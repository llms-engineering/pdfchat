import streamlit as st
import logging
import os
import tempfile
import shutil

from ollama import Client

from langchain_community.document_loaders import UnstructuredPDFLoader
from langchain_community.embeddings import OllamaEmbeddings
from langchain_openai import OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_qdrant import QdrantVectorStore
from langchain.prompts import ChatPromptTemplate, PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_community.chat_models import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_core.runnables import RunnablePassthrough
from langchain.retrievers.multi_query import MultiQueryRetriever


from typing import List, Tuple, Dict, Any, Optional

from dotenv import load_dotenv

# åŠ è½½.envæ–‡ä»¶
load_dotenv()

# Streamlité¡µé¢é…ç½®
st.set_page_config(
    page_title="Pdfé—®ç­”ç¤ºä¾‹",
    page_icon="ğŸˆ",
    layout="wide",
    initial_sidebar_state="expanded",
)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

logger = logging.getLogger(__name__)


@st.cache_resource(show_spinner=True)
def extract_model_names(
    models_info: Dict[str, List[Dict[str, Any]]],
) -> Tuple[str, ...]:
    """
    ä»ollamaä¸­è·å–æ¨¡å‹åç§°

    å‚æ•°:
        models_info (Dict[str, List[Dict[str, Any]]]): å­—å…¸ä¸­åŒ…å«ollamaè¿”å›çš„æ¨¡å‹ä¿¡æ¯

    è¿”å›:
        Tuple[str, ...]: æ¨¡å‹åç§°çš„å…ƒç»„.
    """
    logger.info("ä»Ollamaä¸­è·å–æ¨¡å‹åç§°")
    model_names = tuple(model["name"] for model in models_info["models"])
    embedding_model = os.getenv('embedding_model')
    model_names = [model for model in model_names if model!= embedding_model]
    logger.info(f"è·å–çš„æ¨¡å‹åç§°ä¸º: {model_names}")
    return model_names


def create_vector_db(file_upload,provider) -> QdrantVectorStore:
    """
    æ ¹æ®ä¸Šä¼ çš„PDFæ–‡ä»¶åˆ›å»ºå‘é‡æ•°æ®åº“

    å‚æ•°:
        file_upload (st.UploadedFile): ä½¿ç”¨Streamlitæ–‡ä»¶ä¸Šä¼ ç»„ä»¶.

    è¿”å›:
        QdrantVectorStore: æ„å»ºå®Œæˆçš„å‘é‡æ•°æ®åº“.
    """
    logger.info(f"æ­£åœ¨ä¸Šä¼ æ–‡ä»¶: {file_upload.name}")
    temp_dir = tempfile.mkdtemp()

    path = os.path.join(temp_dir, file_upload.name)
    
    with open(path, "wb") as f:
        f.write(file_upload.getvalue())
        logger.info(f"ä¸´æ—¶æ–‡ä»¶å­˜å‚¨è·¯å¾„: {path}")
        loader = UnstructuredPDFLoader(path)
        data = loader.load()

    text_splitter = RecursiveCharacterTextSplitter(chunk_size=700, chunk_overlap=100)
    chunks = text_splitter.split_documents(data)
    logger.info("æ–‡æ¡£åˆ†å—å®Œæˆ")

    if provider == "äº‘":
        embeddings = OpenAIEmbeddings(base_url=os.getenv('openai_url'))
    else:
        embeddings = OllamaEmbeddings(base_url=os.getenv('ollama_url'),model=os.getenv('embedding_model'), show_progress=True)
    
    vector_db = QdrantVectorStore.from_documents(
        chunks,
        embeddings,
        url= os.getenv('qdrant_url'),
        prefer_grpc=True,
        force_recreate=True,
        collection_name="pdfchat",
    )
    logger.info("å‘é‡æ•°æ®åº“æ„å»ºå®Œæˆ")

    shutil.rmtree(temp_dir)
    logger.info(f"æ¸…é™¤ä¸´æ—¶æ–‡ä»¶ç›®å½• {temp_dir}")
    return vector_db


def rag_process(question: str, vector_db: QdrantVectorStore, selected_model: str,temperature:float,provider: str) -> str:
    """
    åŸºäºå‘é‡æ£€ç´¢çš„RAGè¿‡ç¨‹

    å‚æ•°:
        question (str): ç”¨æˆ·è¾“å…¥çš„é—®é¢˜.
        vector_db (QdrantVectorStore): å‘é‡æ•°æ®åº“.
        selected_model (str): é€‰æ‹©çš„LLM.

    è¿”å›:
        str: RAGç”Ÿæˆçš„ç­”æ¡ˆ.
    """
    logger.info(f"""ç”¨æˆ·çš„é—®é¢˜: {question} é€‰æ‹©çš„æ¨¡å‹: {selected_model}""")
    if provider == "äº‘":
        llm = ChatOpenAI(base_url=os.getenv('openai_url'),model=selected_model,temperature=temperature);
    else:
        llm = ChatOllama(base_url=os.getenv('ollama_url'),model=selected_model, temperature=temperature)
    QUERY_PROMPT = PromptTemplate(
        input_variables=["question"],
        template="""ä½ æ˜¯ä¸€åAIè¯­è¨€æ¨¡å‹åŠ©ç†ã€‚æ‚¨çš„ä»»åŠ¡æ˜¯ç”Ÿæˆç»™å®šç”¨æˆ·é—®é¢˜çš„3ä¸ªä¸åŒç‰ˆæœ¬ä¸­æ–‡é—®é¢˜ï¼Œä»¥ä»å‘é‡æ•°æ®åº“ä¸­æ£€ç´¢ç›¸å…³æ–‡æ¡£ã€‚é€šè¿‡å¯¹ç”¨æˆ·é—®é¢˜ç”Ÿæˆå¤šä¸ªè§†è§’çš„é—®é¢˜ï¼Œæ‚¨çš„ç›®æ ‡æ˜¯å¸®åŠ©ç”¨æˆ·å…‹æœåŸºäºè·ç¦»çš„ç›¸ä¼¼æ€§æœç´¢çš„ä¸€äº›å±€é™æ€§ã€‚è¯·æä¾›è¿™äº›ç”¨æ¢è¡Œç¬¦åˆ†éš”çš„å¤‡é€‰é—®é¢˜ã€‚
        åŸå§‹é—®é¢˜: {question}""",
    )

    retriever = MultiQueryRetriever.from_llm(
        vector_db.as_retriever(), llm, prompt=QUERY_PROMPT
    )

    template = """ä»…æ ¹æ®ä»¥ä¸‹ä¸Šä¸‹æ–‡ä½¿ç”¨ä¸­æ–‡è¨€ç®€æ„èµ…å›ç­”é—®é¢˜ï¼š
    {context}
    é—®é¢˜: {question}
    å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œå°±è¯´ä½ ä¸çŸ¥é“ï¼Œä¸è¦è¯•å›¾ç¼–é€ ç­”æ¡ˆã€‚
    åªæä¾›{context}ä¸­çš„ç­”æ¡ˆï¼Œå…¶ä»–ä»€ä¹ˆéƒ½ä¸æä¾›ã€‚
    æ·»åŠ æ‚¨ç”¨æ¥å›ç­”é—®é¢˜çš„ä¸Šä¸‹æ–‡ç‰‡æ®µã€‚
    """

    prompt = ChatPromptTemplate.from_template(template)

    chain = (
        {"context": retriever, "question": RunnablePassthrough()}
        | prompt
        | llm
        | StrOutputParser()
    )

    response = chain.invoke(question)
    logger.info("LLM ç”Ÿæˆå®Œæˆ")
    return response


def delete_vector_db(vector_db: Optional[QdrantVectorStore]) -> None:
    """
    åˆ é™¤å‘é‡æ•°æ®åº“å¹¶æ¸…é™¤ç›¸å…³ä¼šè¯çŠ¶æ€.

    å‚æ•°:
        vector_db (Optional[QdrantVectorStore]): è¦åˆ é™¤çš„å‘é‡æ•°æ®åº“.
    """
    logger.info("åˆ é™¤å‘é‡æ•°æ®åº“")
    if vector_db is not None:
        vector_db.delete_collection()
        st.session_state.pop("pdf_pages", None)
        st.session_state.pop("file_upload", None)
        st.session_state.pop("vector_db", None)
        st.success("æ•°æ®åº“è¡¨æ¸…ç†å®Œæˆ")
        logger.info("æ•°æ®åº“è¡¨æ¸…ç†å®Œæˆ")
        st.rerun()
    else:
        st.error("æ²¡æœ‰æ‰¾åˆ°å¯åˆ é™¤çš„å‘é‡æ•°æ®åº“.")
        logger.warning("æ²¡æœ‰æ‰¾åˆ°å¯åˆ é™¤çš„å‘é‡æ•°æ®åº“.")


def main() -> None:
    """
    ä¸»ç¨‹åºï¼Œè´Ÿè´£é¡µé¢å¸ƒå±€å’Œäº¤äº’é€»è¾‘
    """
    st.title("ğŸ¦œğŸ”— Pdfé—®ç­”", anchor=False)
    user_avator = "ğŸ§‘â€ğŸ’»"
    robot_avator = "ğŸ¤–"

    
    col1, col2 = st.columns([3, 5])

    if "messages" not in st.session_state:
        st.session_state["messages"] = []

    if "vector_db" not in st.session_state:
        st.session_state["vector_db"] = None


    provider = col1.radio("æ¨¡å‹æœåŠ¡", ["äº‘","æœ¬åœ°"],horizontal=True)
    if provider == 'æœ¬åœ°':
        try:
             client = Client(host=os.getenv('ollama_url'))
             models_info = client.list()
             available_models = extract_model_names(models_info)
             selected_model = col1.selectbox("æ¨¡å‹é€‰æ‹©", available_models)
        except Exception as e:
            print(e)
            col1.error("è¯·æ£€æŸ¥ollamaæœåŠ¡æ˜¯å¦è¿è¡Œæ­£ç¡®") 
    else:
        available_models = ('gpt-3.5-turbo', 'gpt-4-turbo','gpt-4o')
        selected_model = col1.selectbox("æ¨¡å‹é€‰æ‹©", available_models)
        api_key = col1.text_input("OpenAI_API_Key", "", type="password")


    temperature = col1.slider('æ¸©åº¦(Temperature)', 0.0, 1.0, 0.0, step=0.1)
    file_upload = col1.file_uploader(
        "ä¸Šä¼ å¾…é—®ç­”çš„Pdf", type="pdf", accept_multiple_files=False
    )
    status_placeholder = col1.empty()

    if provider == "äº‘" and (api_key is not None and api_key.startswith("sk-") and st.session_state.get('api_key') is None):
        os.environ["OPENAI_API_KEY"] = api_key
        status_placeholder.info("keyè®¾ç½®å®Œæˆ")
        st.session_state["api_key"] = True
        
    if file_upload:
        st.session_state["file_upload"] = file_upload
        if st.session_state["vector_db"] is None:
            status_placeholder.info("çŸ¥è¯†åº“æ„å»ºä¸­ï¼Œè¯·ç¨å...")
            st.session_state["vector_db"] = create_vector_db(file_upload,provider)
            status_placeholder.info("çŸ¥è¯†åº“å°±ç»ªï¼Œå¯ä»¥æé—®äº†!")

    delete_collection = col1.button("åˆ é™¤è¿æ¥", type="secondary")

    if delete_collection:
        delete_vector_db(st.session_state["vector_db"])

    with col2:
        message_container = st.container(height=500, border=True)

        for message in st.session_state["messages"]:
            avatar = robot_avator if message["role"] == "assistant" else user_avator
            with message_container.chat_message(message["role"], avatar=avatar):
                st.markdown(message["content"])

        if prompt := st.chat_input("è¾“å…¥ä½ çš„é—®é¢˜..."):
            try:
                st.session_state["messages"].append({"role": "user", "content": prompt})
                message_container.chat_message("user", avatar=user_avator).markdown(prompt)

                with message_container.chat_message("assistant", avatar=robot_avator):
                    with st.spinner(":green[processing...]"):
                        if st.session_state["vector_db"] is not None :
                            response = rag_process(
                                prompt, st.session_state["vector_db"], selected_model,temperature,provider
                            )
                            st.markdown(response)
                        else:
                            st.warning("ä½ è¿˜æ²¡æœ‰ä¸Šä¼ æ–‡æ¡£å‘¢!")

                if st.session_state["vector_db"] is not None:
                    st.session_state["messages"].append(
                        {"role": "assistant", "content": response}
                    )

            except Exception as e:
                st.error(e)
                logger.error(f"Ragæ‰§è¡Œæ—¶å‘ç”Ÿé”™è¯¯: {e}")
        else:
            if provider == "äº‘" and st.session_state.get('api_key') is None:
                status_placeholder.warning("è¿˜æœªè®¾ç½®OpenAI API key...")
            elif st.session_state["vector_db"] is None:
                status_placeholder.warning("æ–‡æ¡£è¿˜æœªä¸Šä¼ ...")



if __name__ == "__main__":
    main()
